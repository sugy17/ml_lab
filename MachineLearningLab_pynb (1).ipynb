{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MachineLearningLab.pynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubdoi9KTgHXR",
        "cellView": "form"
      },
      "source": [
        "#@title pgm1 { form-width: \"5%\" }\n",
        "# \"\"\"\n",
        "# 1. Implement and demonstrate the FIND-S algorithm for finding the most specific\n",
        "# hypothesis based on a given set of training data samples. Read the training data from a\n",
        "# .CSV file.\n",
        "\n",
        "# Algo:\n",
        "# 1. Load dataset\n",
        "# 2. Initiliase h to most specific hyp in H.\n",
        "# 3. for each positive training instance x:\n",
        "#         for each attribute constraint ai in x:\n",
        "#             if the constraint is satisfied by x then do nothing\n",
        "#             else replace ai in h with next most general constraint that is satisfied by x\n",
        "# 4. display h\n",
        "# \"\"\"\n",
        "\n",
        "import csv\n",
        "data = []\n",
        "\n",
        "with open('dataset1.csv', 'r') as csvFile:\n",
        "    reader = csv.reader(csvFile)\n",
        "    data = list(reader)\n",
        "    data.pop(0)  # removing headers from dataset\n",
        "num_attributes = len(data[0])-1 # we don't want last col which is target concet ( yes/no)\n",
        "\n",
        "print(\"The training data is as follows\")\n",
        "print(data)\n",
        "S = ['phi'] * 6\n",
        "print (\"\\nThe most specific hypothesis :\",S,\"\\n\")\n",
        "print (\"\\nThe most general hypothesis : ['?','?','?','?','?','?']\\n\")\n",
        "\n",
        "S = data[0][:-1].copy();\n",
        "\n",
        "for i in range(len(data)):\n",
        "    if data[i][-1] == 'Yes':\n",
        "        for j in range(num_attributes):\n",
        "            if S[j] != data[i][j]:\n",
        "                S[j] = '?'\n",
        "    print(\"\\nFor Training example \", i ,\"the hypothesis is \" , S)\n",
        "print(\"\\n The Maximally Specific Hypothesis for a given Training Examples :\\n\\n\",S)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7VHnEOextMS",
        "cellView": "form"
      },
      "source": [
        "#@title pgm2 { form-width: \"5%\" }\n",
        "\n",
        "# Algo\n",
        "# Step1: Load Data set\n",
        "# Step2: Initialize General Hypothesis  and Specific  Hypothesis.\n",
        "# Step3: For each training example\n",
        "# Step4: If example is positive example\n",
        "#           if attribute_value == hypothesis_value:\n",
        "#              Do nothing\n",
        "#           else:\n",
        "#              replace attribute value with '?' (Basically generalizing it)\n",
        "# Step5: If example is Negative example\n",
        "#           Make generalize hypothesis more specific.\n",
        "\n",
        "import csv\n",
        "data = []\n",
        "print(\"\\n The Given Training Data Set \\n\")\n",
        "\n",
        "with open('dataset1.csv', 'r') as csvFile:\n",
        "    reader = csv.reader(csvFile)\n",
        "    data = list(reader)\n",
        "    data.pop(0)\n",
        "num_attributes = len(data[0])-1 # we don't want last col which is target concet ( yes/no)\n",
        "\n",
        "print(data)\n",
        "S = ['phi'] * num_attributes\n",
        "temp = ['?'] * num_attributes\n",
        "print (\"\\n The most specific hypothesis S0 : \",S,\"\\n\")\n",
        "print (\" \\n The most general hypothesis G0 : \",G,\"\\n\")\n",
        "\n",
        "S = data[0][:-1].copy();\n",
        "# Comparing with Remaining Training Examples of Given Data Set\n",
        "\n",
        "print(\"\\n Candidate Elimination algorithm  Hypotheses Version Space Computation\\n\")\n",
        "G=[]\n",
        "\n",
        "for i in range(0,len(data)):\n",
        "    if data[i][-1]=='Yes':\n",
        "        for j in range(0,num_attributes):\n",
        "            if data[i][j]!=S[j]:\n",
        "                S[j]='?'\n",
        "\n",
        "        for j in range(0,num_attributes):\n",
        "            for k  in range(0,len(G)):\n",
        "                if G[k][j] != '?' and G[k][j] != S[j]:\n",
        "                    del G[k] #remove it if it's not matching with the specific hypothesis\n",
        "\n",
        "    else: \n",
        "        for j in range(0,num_attributes):\n",
        "             if S[j] != data[i][j] and S[j]!= '?':  #if not  matching with the specific Hypothesis take it seperately and store it \n",
        "                 temp[j]=S[j]\n",
        "                 G.append(temp) # this is the version space to store all Hypotheses\n",
        "                 temp = ['?'] * num_attributes\n",
        "    \n",
        "    print(\"S\",i+1,\": \",S)\n",
        "    \n",
        "    if (len(G)==0):\n",
        "        print(\"G\",i+1,\": \",temp)\n",
        "    else:\n",
        "         print(\"G\",i+1,\": \",G)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOSt-DSP5_y-",
        "cellView": "form"
      },
      "source": [
        "#@title pgm3 { form-width: \"5%\" }\n",
        "import csv\n",
        "import pandas as pd\n",
        "from math import log2\n",
        "import operator\n",
        "from pandas.core.frame import DataFrame\n",
        "\n",
        "\n",
        "def loadCsv(filename):\n",
        "    with open(filename) as f:\n",
        "        data = csv.reader(f)\n",
        "        lines = list(data)\n",
        "    return lines\n",
        "\n",
        "\n",
        "data = loadCsv(\"dataset.csv\")\n",
        "attrs = data[0]\n",
        "vals = data[1:]\n",
        "d = pd.DataFrame(vals, columns=attrs)\n",
        "\n",
        "\n",
        "def entropy(P, N):\n",
        "    if P == 0 or N == 0:\n",
        "        return 0\n",
        "    # Formula for Entropy:\n",
        "    entr_p = ((-P) / (P + N)) * log2((P) / (P + N))\n",
        "    entr_n = ((-N) / (P + N)) * log2((N) / (P + N))\n",
        "    entr = entr_p + entr_n\n",
        "    return entr\n",
        "\n",
        "\n",
        "def entropy_dataset(data):\n",
        "    f = data[\"PlayTennis\"]\n",
        "    P = sum(int(x == \"Yes\") for x in f)\n",
        "    N = len(f) - P\n",
        "    return (entropy(P, N), P, N)\n",
        "\n",
        "\n",
        "def entropy_attr(data: DataFrame, P, N):\n",
        "\n",
        "    cols = data.columns[:-1]\n",
        "    entropy_dict = {K: None for K in cols}\n",
        "\n",
        "    for col in cols:\n",
        "        coldata = data[[col, \"PlayTennis\"]]\n",
        "        dset = {K: [0, 0] for K in set(coldata[col])}\n",
        "        eset = {K: None for K in set(coldata[col])}\n",
        "\n",
        "        # Count Sub-Attr Against Yes or No from PlayTennis\n",
        "        for (value, outcome) in coldata.itertuples(index=False, name=None):\n",
        "            if outcome == \"Yes\":\n",
        "                dset[value][0] += 1\n",
        "            else:\n",
        "                dset[value][1] += 1\n",
        "\n",
        "        # Calculate Entropy for Each, Formula.\n",
        "        for (subattr, calc) in dset.items():\n",
        "            p = (calc[0] + calc[1]) / (P + N)\n",
        "            p = p * entropy(calc[0], calc[1])\n",
        "            eset[subattr] = p\n",
        "\n",
        "        # Finally, Push the Sum into the Main Array.\n",
        "        entropy_dict[col] = sum(eset.values())\n",
        "\n",
        "    return entropy_dict\n",
        "\n",
        "# Gain Formula\n",
        "def gain(data_entropy, attr_entropy):\n",
        "    gain = {K: data_entropy - attr_entropy[K] for K in attr_entropy.keys()}\n",
        "    return gain\n",
        "\n",
        "\n",
        "def create_tree(data, name, indent):\n",
        "\n",
        "    # Check if Each is just Yes or No\n",
        "    if all(x == \"Yes\" for x in data[\"PlayTennis\"]):\n",
        "        print(\"{} Yes\".format(indent, name))\n",
        "        return\n",
        "\n",
        "    if all(x == \"No\" for x in data[\"PlayTennis\"]):\n",
        "        print(\"{} No\".format(indent, name))\n",
        "        return\n",
        "\n",
        "    # Calculate for Each step of the Way.\n",
        "    entrd, P, N = entropy_dataset(data)\n",
        "    avg_info_entropy = entropy_attr(data, P, N)\n",
        "    gain_data = gain(entrd, avg_info_entropy)\n",
        "\n",
        "    # Just get the Max value from gain_data.\n",
        "    max_val = max(gain_data.items(), key=operator.itemgetter(1))[0]\n",
        "\n",
        "    print(indent + max_val)\n",
        "    indent += \"  \"\n",
        "\n",
        "    # Do this groupby\n",
        "    grouped = data.groupby(max_val)\n",
        "\n",
        "    for group in grouped.groups.keys():\n",
        "        print(indent + group)\n",
        "        new_table = grouped.get_group(group)\n",
        "        create_tree(new_table, group, indent + \"  \")\n",
        "\n",
        "\n",
        "create_tree(d, \"root\", \"\")\n",
        "\n",
        "# \"\"\"\n",
        "# Outlook\n",
        "#   Overcast\n",
        "#      Yes\n",
        "#   Rainy\n",
        "#     Windy\n",
        "#       False\n",
        "#          Yes\n",
        "#       True\n",
        "#          No\n",
        "#   Sunny\n",
        "#     Humidity\n",
        "#       High\n",
        "#          No\n",
        "#       Normal\n",
        "#          Yes\n",
        "# \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWyrOdv9xbi-",
        "cellView": "form"
      },
      "source": [
        "#@title pgm4 { form-width: \"5%\" }\n",
        "import numpy as np \n",
        "X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float) # Features ( Hrs Slept, Hrs Studied)\n",
        "y = np.array(([92], [86], [89]), dtype=float) # Labels(Marks obtained)\n",
        "c=np.amax(X,axis=0) # Normalize\n",
        "print(c)\n",
        "X = X/c # Normalize\n",
        "y = y/100\n",
        "print(X)\n",
        "print(y)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1/(1 + np.exp(-x))\n",
        "def sigmoid_grad(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        " # Variable initialization\n",
        "epoch=1000 #Setting training iterations\n",
        "eta =0.1 #Setting learning rate (eta)\n",
        "input_neurons = 2 #number of features in data set\n",
        "hidden_neurons = 3 #number of hidden layers neurons\n",
        "output_neurons = 1 #number of neurons at output layer\n",
        "# Weight and bias - Random initialization\n",
        "wh=np.random.uniform(size=(input_neurons,hidden_neurons)) # 2x3\n",
        "print(wh)\n",
        "bh=np.random.uniform(size=(1,hidden_neurons)) # 1x3\n",
        "print(bh)\n",
        "wout=np.random.uniform(size=(hidden_neurons,output_neurons)) # 3x1\n",
        "print(wout)\n",
        "bout=np.random.uniform(size=(1,output_neurons))\n",
        "print(bout)\n",
        "\n",
        "for i in range(epoch):\n",
        "    #Forward Propogation\n",
        "    h_ip=np.dot(X,wh) + bh # Dot product + bias\n",
        "    print(h_ip)\n",
        "    h_act = sigmoid(h_ip) # Activation function\n",
        "    o_ip=np.dot(h_act,wout) + bout\n",
        "    output = sigmoid(o_ip)\n",
        "    \n",
        "    #Backpropagation\n",
        "    # Error at Output layer\n",
        "    Eo = y-output # Error at o/p\n",
        "    outgrad = sigmoid_grad(output)\n",
        "    d_output = Eo* outgrad # Errj=Oj(1-Oj)(Tj-Oj)\n",
        "    print(\"the d_output is\",d_output)\n",
        "\n",
        "    # Error at Hidden later\n",
        "    Eh = d_output.dot(wout.T) # .T means transpose\n",
        "    hiddengrad = sigmoid_grad(h_act) # How much hidden layer wts contributed to error\n",
        "    d_hidden = Eh * hiddengrad\n",
        "    wout += h_act.T.dot(d_output) *eta # Dotproduct of nextlayererror and currentlayerop\n",
        "    wh += X.T.dot(d_hidden) *eta\n",
        "\n",
        "print(\"Normalized Input: \\n\" + str(X))\n",
        "print(\"Actual Output: \\n\" + str(y))\n",
        "print(\"Predicted Output: \\n\" ,output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8JBFc8JBz51",
        "outputId": "38f8ee81-ac4f-4754-ef79-f7436d502b41"
      },
      "source": [
        "#@title pgm5 { form-width: \"5%\" }\n",
        "\n",
        "# for gausian baye dataset only, skipped binomial baye\n",
        "import math\n",
        "import pandas as pd\n",
        "from functools import partial\n",
        "\n",
        "#filename = 'pima-indians-diabetes.csv'\n",
        "data = pd.read_csv(\"pima-indians-diabetes.csv\")\n",
        "data.columns = [\n",
        "    \"Pregnancies\",\n",
        "    \"Glucose\",\n",
        "    \"BloodPressure\",\n",
        "    \"SkinThickness\",\n",
        "    \"Insulin\",\n",
        "    \"BMI\",\n",
        "    \"DiabetesPedigreeFunction\",\n",
        "    \"Age\",\n",
        "    \"Class\",\n",
        "]\n",
        "\n",
        "# Gauissian Bayes.\n",
        "# Class is the Deciding Factor\n",
        "def GaussianFunction(mean, std, x):\n",
        "    return (1 / (math.sqrt(2 * math.pi) * std)) * math.pow(math.e, -math.pow(x - mean, 2) / (2 * std ** 2))\n",
        "\n",
        "\n",
        "def freq_table(data: pd.DataFrame):\n",
        "    vnf = {}\n",
        "    for col in data.columns[:-1]:\n",
        "        values = data[[col, \"Class\"]]\n",
        "        P = values.loc[values[\"Class\"] == 1]\n",
        "        N = values.loc[values[\"Class\"] == 0]\n",
        "        vnf[col] = [\n",
        "            partial(GaussianFunction, P[col].mean(), P[col].std()),\n",
        "            partial(GaussianFunction, N[col].mean(), N[col].std()),\n",
        "        ]\n",
        "    return vnf\n",
        "\n",
        "\n",
        "def pred(data: pd.DataFrame, fq):\n",
        "    predictions = []\n",
        "    for row in data.to_numpy():\n",
        "        Pprob = 1\n",
        "        Nprob = 1\n",
        "        for (col, val) in zip(data.columns, row):\n",
        "            Pprob = Pprob * fq[col][0](val)\n",
        "            Nprob = Nprob * fq[col][1](val)\n",
        "        predictions.append(int(Pprob > Nprob))\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def get_Accuracy(actual, predicted):\n",
        "    count_true = 0\n",
        "    total = len(predicted)\n",
        "    for x, y in zip(actual, predicted):\n",
        "        count_true += (x == y)\n",
        "    return count_true / total\n",
        "\n",
        "\n",
        "ratio = 0.5\n",
        "intratio = int(ratio * len(data))\n",
        "train, test = data[:intratio], data[intratio:]\n",
        "vtable = freq_table(train)\n",
        "prediction = pred(test.drop(columns=\"Class\"), vtable)\n",
        "acc = get_Accuracy(test.iloc[:, -1], prediction)\n",
        "print(\"Acc is \",acc * 100)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acc is  76.30208333333334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Aq2oET9iCe_B"
      },
      "source": [
        "#@title pgm6 { form-width: \"5%\" }\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import sklearn.datasets as skd\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "\n",
        "\n",
        "# Get Some categories of Study\n",
        "categories = [\"alt.atheism\", \"soc.religion.christian\", \"comp.graphics\", \"sci.med\"]\n",
        "\n",
        "# Get Train Dataset for categories\n",
        "twenty_train = fetch_20newsgroups(subset=\"train\", categories=categories, shuffle=True)\n",
        "# Get the Test Dataset for the same.\n",
        "twenty_test = fetch_20newsgroups(subset=\"test\", categories=categories, shuffle=True)\n",
        "\n",
        "# Add in the Local Conditions to, Local Downloads.\n",
        "# news_train=skd.load_files(r\"/home/student/Desktop/ML/20news-bydate-train\",categories=categories,encoding='ISO-8859-1')\n",
        "# news_test=skd.load_files(r\"/home/student/Desktop/ML/20news-bydate-test\",categories=categories,encoding='ISO-8859-1')\n",
        "\n",
        "# Vectorizers\n",
        "cvec = CountVectorizer()\n",
        "tvec = TfidfTransformer()\n",
        "\n",
        "# Vectorize the Data.\n",
        "X_Train = cvec.fit_transform(twenty_train.data)\n",
        "X_Train_TFIDF = tvec.fit_transform(X_Train)\n",
        "\n",
        "# Vectorize the Tests\n",
        "X_Test = cvec.transform(twenty_test.data)\n",
        "X_Test_TFIDF = tvec.transform(X_Test)\n",
        "\n",
        "\n",
        "# Make Model\n",
        "Model = MultinomialNB()\n",
        "\n",
        "# Train Model\n",
        "Model.fit(X_Train_TFIDF, twenty_train.target)\n",
        "\n",
        "# Make Predictions\n",
        "\n",
        "X_Pred = Model.predict(X_Test_TFIDF)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(twenty_test.target, X_Pred))\n",
        "print(\"Confusion matrix is \\n\", confusion_matrix(twenty_test.target, X_Pred))\n",
        "print(classification_report(twenty_test.target, X_Pred, target_names=twenty_test.target_names))\n",
        "\n",
        "# prediction\n",
        "doc=['god is love','openGL on the GPU is fast']\n",
        "X_new_counts=cvec.transform(doc)\n",
        "X_new_tfid=tvec.transform(X_new_counts)\n",
        "predicted=Model.predict(X_new_tfid)\n",
        "for x in predicted:\n",
        "    print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Aba2YZVvA_so"
      },
      "source": [
        "#@title pgm7 { form-width: \"5%\" }\n",
        "! pip install pgmpy\n",
        "import pandas as pd\n",
        "from pgmpy.models import BayesianModel\n",
        "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
        "from pgmpy.inference import VariableElimination\n",
        "heartDisease = pd.read_csv('p.csv', names=['preg', 'glucose', 'bp', 'skinthick', 'insulin', 'bmi', 'diapedigree', 'age', 'class'])\n",
        "print(heartDisease.head())\n",
        "model = BayesianModel([('preg', 'glucose'), ('bp', 'skinthick'),('insulin', 'bmi'), ('bmi', 'diapedigree'), ('age', 'class'), ('insulin', 'class')])\n",
        "model.fit(heartDisease, estimator=MaximumLikelihoodEstimator)\n",
        "print(model.get_cpds('preg'))\n",
        "heart_infer = VariableElimination(model)\n",
        "print(\"Probability of heart disease given age = 28\\n\")\n",
        "q = heart_infer.query(variables=['class'], evidence={'age': 28})\n",
        "print(q)\n",
        "print(\"Probability of heart disease given bp = 60\\n\")\n",
        "q = heart_infer.query(variables=['class'], evidence={'insulin': 60})\n",
        "print(q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FlVFojtjBI7f"
      },
      "source": [
        "#@title pgm8 { form-width: \"5%\" }\n",
        "from sklearn import datasets\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#%matplotlib inline\n",
        "\n",
        "# import some data to play with\n",
        "iris = datasets.load_iris()\n",
        "print(\"\\n IRIS FEATURES :\\n\",iris.feature_names)\n",
        "print(\"\\n IRIS TARGET :\\n\",iris.target)\n",
        "print(\"\\n IRIS TARGET NAMES:\\n\",iris.target_names)\n",
        "\n",
        "# Store the input\n",
        "# This is what KMeans thought model.labels_ as a Pandas Dataframe and set the column names\n",
        "X = pd.DataFrame(iris.data)\n",
        "X.columns = ['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width']\n",
        "y = pd.DataFrame(iris.target)\n",
        "y.columns = ['Targets']\n",
        "\n",
        "# Set the size of the plot and Create a colormap\n",
        "plt.figure(figsize=(14,7))\n",
        "colormap = np.array(['red', 'lime', 'black'])\n",
        " \n",
        "# Plot Orginal\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X.Petal_Length, X.Petal_Width, c=colormap[y.Targets], s=40)\n",
        "plt.title('Real Classification')\n",
        "\n",
        "# Build model\n",
        "# K Means Cluster\n",
        "km = KMeans(n_clusters=3)\n",
        "predY_km = km.fit_predict(X)\n",
        "# Rearrange based on what output you get from realistic classification\n",
        "predY_km = np.choose(predY_km, [2, 0, 1]).astype(np.int64)\n",
        "plt.figure(figsize=(14,7))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X.Petal_Length,X.Petal_Width, c=colormap[predY_km], s=40)\n",
        "plt.title('K Mean Classification')\n",
        "\n",
        "# Performance Metrics\n",
        "print(accuracy_score(y, predY_km))\n",
        "# Confusion Matrix\n",
        "print(confusion_matrix(y, predY_km))\n",
        "\n",
        "\n",
        "# gausianmodel\n",
        "gmm = GaussianMixture(n_components=3)\n",
        "predY_gmm = gmm.fit_predict(X)\n",
        "\n",
        "# Rearrange based on what output you get from realistic classification\n",
        "predY_gmm = np.choose(predY_gmm, [1, 0, 2]).astype(np.int64)\n",
        "plt.figure(figsize=(14,7))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X.Petal_Length, X.Petal_Width, c=colormap[predY_gmm], s=40)\n",
        "plt.title('GMM Classification')\n",
        "\n",
        "print(accuracy_score(y, predY_gmm))\n",
        "print(confusion_matrix(y, predY_gmm))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "0pTfw4GNBNrb"
      },
      "source": [
        "#@title pgm9 { form-width: \"5%\" }\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#load iris dataset\n",
        "from sklearn.datasets import load_iris\n",
        "iris=load_iris()\n",
        "#print(iris)\n",
        "\n",
        "iris=datasets.load_iris()\n",
        "iris_data=iris.data\n",
        "iris_labels=iris.target\n",
        "\n",
        "x_train, x_test, y_train, y_test= train_test_split(iris_data, iris_labels, test_size=0.20)\n",
        "classifier=KNeighborsClassifier(n_neighbors=6)\n",
        "classifier.fit(x_train, y_train)\n",
        "y_pred=classifier.predict(x_test)\n",
        "\n",
        "print(\"condusion matrix:\\n\",confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nreport:\\n\",classification_report(y_test, y_pred))\n",
        "\n",
        "# Print which one was correct, which one was wrong\n",
        "d = {\"Y Test\": y_test, \"Y Predicted\": y_pred, \"Is Correct?\":[]}\n",
        "\n",
        "for x,y in zip(y_test, y_pred):\n",
        "    d[\"Is Correct?\"].append(\"Yes\" if x==y else \"No\")\n",
        "\n",
        "final = pd.DataFrame(d)\n",
        "\n",
        "print(final)\n",
        "#\n",
        "# The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
        "\n",
        "# The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
        "\n",
        "# The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "GFBsGzO2BTkK"
      },
      "source": [
        "#@title pgm10 { form-width: \"5%\" }\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "tou = 0.8\n",
        "\n",
        "X_train =  np.array(list(range(3,33))+[3.2,4.2])[:,None] #np.newaxis # convert 1d to 2d\n",
        "y_train=np.array([1,2,1,2,1,1,3,4,5,4,5,6,5,6,7,8,9,10,11,11,12,11,11,12,13,16,17,19,18,34,21,22])\n",
        "\n",
        "X_test = np.array([i/10. for i in range(340)])[:,None]   #np.newaxis # convert 1d to 2d\n",
        "y_test = []\n",
        "\n",
        "for r in X_test:\n",
        "    wt = np.exp(-np.sum((X_train-r)**2,axis=1)/(2*tou)**2)\n",
        "    w = np.diag(wt)\n",
        "    teth1 = np.linalg.inv(X_train.T.dot(w).dot(X_train))\n",
        "    teth2 = X_train.T.dot(w).dot(y_train)\n",
        "    prediction = r.dot(teth1).dot(teth2)\n",
        "    y_test.append(prediction)\n",
        "\n",
        "plt.plot(X_train.squeeze(), y_train, 'o')\n",
        "plt.plot(X_test.squeeze(), np.array(y_test), 'r-')\n",
        "#print(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXAmBiHH0F9l",
        "cellView": "form"
      },
      "source": [
        "#@title mount gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}